{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_mistral_api_key, get_text_embedding\n",
    "import numpy as np\n",
    "\n",
    "api_key = load_mistral_api_key(ret_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap the content of a page\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/\"\n",
    ")\n",
    "html_doc = response.text\n",
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "tag = soup.find(\"div\", re.compile(\"^prose--styled\"))\n",
    "text = tag.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) save the the content in a text file\n",
    "file_name = \"AI_greenhouse_gas.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : chunk the text into 512 characters chunks\n",
    "chunk_size = 512\n",
    "chunks = [text[i: i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "print(f\"we have {len(chunks)} chunks of 512 characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : embed the chunks using mistral embedding API\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(text_embeddings[0])\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : store the embeddings in a vector database (using Faiss)\n",
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "\n",
    "embedding_dim = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # build the index\n",
    "index.add(text_embeddings)  # add the embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Embed the query using the same embedding model (mistral embedding)\n",
    "question = \"What are the ways that AI can reduce emissions in Agriculture?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 : Search the most similar chunks to the query\n",
    "# D : distance , I : index of the k most similar chunks to the query vector\n",
    "D, I = index.search(question_embeddings, k=2)\n",
    "print(f\"The 2 most similar chunks to the query are :\", I)\n",
    "print(f\"The distances between the query and the 2 most similar chunks are : \", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 : Retrieve the most similar chunks using their index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 : Pass the retrieved chunks in the prompt to get the answer to the question\n",
    "from helper import mistral\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with function calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG complete basic method (split into chunk, embed, load into vector database, retrieve similar chunks, generate response)\n",
    "def qa_with_context(text, question, chunk_size=512):\n",
    "    # split document into chunks\n",
    "    chunks = [text[i: i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # load into a vector database\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieve relevant text chunks\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Context information is below.\n",
    "    ---------------------\n",
    "    {retrieved_chunk}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Query: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = mistral(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define QA with context as a tool\n",
    "import functools\n",
    "\n",
    "names_to_functions = {\n",
    "    \"qa_with_context\": functools.partial(qa_with_context, text=text)\n",
    "}\n",
    "\n",
    "qa_with_context_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "            \"name\": \"qa_with_context\",\n",
    "            \"description\": \"Answer user question by retrieving relevant context\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"user question\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"],\n",
    "            },\n",
    "    },\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    qa_with_context_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat with the new tool\n",
    "\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import os\n",
    "\n",
    "question = \"\"\"\n",
    "What are the ways AI can mitigate climate change in transportation?\n",
    "\"\"\"\n",
    "\n",
    "client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "print(client)\n",
    "\n",
    "\n",
    "response = client.chat(\n",
    "    model=\"mistral-small-latest\",\n",
    "    messages=[ChatMessage(role=\"user\", content=question)],\n",
    "    tools=tools,\n",
    "    tool_choice=\"any\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tool function called and the arguments to call the function with\n",
    "import json\n",
    "\n",
    "tool_function = response.choices[0].message.tool_calls[0].function\n",
    "print(\"tool function object: \", tool_function)\n",
    "print\n",
    "tool_function_name = tool_function.name\n",
    "tool_function_arguments = json.loads(tool_function.arguments)\n",
    "print(f\"Tool function called: {tool_function_name}\")\n",
    "print(f\"Tool function arguments : {tool_function_arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the tool function with the arguments to get the response\n",
    "function_result = names_to_functions[tool_function_name](\n",
    "    **tool_function_arguments)\n",
    "function_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
