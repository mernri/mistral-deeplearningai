{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import os\n",
    "import panel as pn\n",
    "from helper import load_mistral_api_key\n",
    "api_key = load_mistral_api_key(ret_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to initiate the chat with Mistral\n",
    "def run_mistral(contents, user, chat_interface):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    messages = [ChatMessage(role=\"user\", content=contents)]\n",
    "    chat_response = client.chat(\n",
    "        model=\"mistral-small-latest\",\n",
    "        messages=messages)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the chat interface\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=run_mistral,\n",
    "    callback_user=\"Mistral\"\n",
    ")\n",
    "\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat + rag tool UI\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "client = MistralClient(\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_text_embedding\n",
    "\n",
    "file_name = \"AI_greenhouse_gas.txt\"\n",
    "with open(file_name, 'r') as file:\n",
    "    file_input = file.read()\n",
    "\n",
    "\n",
    "def answer_question(question, user, instance):\n",
    "    text = file_input.value.decode(\"utf-8\")\n",
    "\n",
    "    # split document into chunks\n",
    "    chunk_size = 2048\n",
    "    chunks = [text[i: i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # load into a vector database\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieved relevant text chunks\n",
    "    response = run_mistral(\n",
    "        prompt.format(retrieved_chunk=retrieved_chunk, question=question)\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_input = pn.widgets.FileInput(accept=\".txt\", value=\"\", height=50)\n",
    "\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=answer_question,\n",
    "    callback_user=\"Mistral\",\n",
    "    header=pn.Row(file_input, \"### Upload a text file to chat with it!\"),\n",
    ")\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mistral!\",\n",
    "    user=\"System\",\n",
    "    respond=False\n",
    ")\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
